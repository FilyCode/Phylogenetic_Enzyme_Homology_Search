{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from Bio import Entrez, SeqIO\n",
    "import re\n",
    "\n",
    "input_csv = r\"C:\\Users\\poker\\Downloads\\big-search_no-filters-40%similarity_70-80%nodes\\OvoA_10k_Blast_search_noFilter_80AST_80%_SSN_leftSenAClusterIDs.csv\"\n",
    "output_csv = \"protein_synteny_leftSenAcluster_output.csv\"\n",
    "\n",
    "Entrez.email = \"example@gmail.com\" # Set email for NCBI Entrez, for responsible use needed\n",
    "search_genes = ['selA', 'selB', 'selC', 'selD', 'SenA', 'SenB', 'SenC', 'SenD', 'EgtA', 'EgtB', 'EgtC', 'EgtD', 'EgtE', 'OvoA']\n",
    "\n",
    "CHECKPOINT_EVERY = 20  # How often to write intermediate result tp CSVs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniprot_mappings(uniprot_id):\n",
    "    \"\"\"Return RefSeq protein, RefSeq nucleotide, EMBL nucleotide, EMBL protein for a UniProt ID.\"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    r = requests.get(url, timeout=10)\n",
    "    if r.status_code != 200:\n",
    "        return None, None, None, None\n",
    "    data = r.json()\n",
    "    refseq_prot, refseq_nt, embl_nt, embl_prot = None, None, None, None\n",
    "    for dbref in data.get('uniProtKBCrossReferences', []):\n",
    "        if dbref['database'] == \"RefSeq\":\n",
    "            refseq_prot = dbref.get('id') or refseq_prot\n",
    "            for prop in dbref.get('properties', []):\n",
    "                if prop['key'] == 'NucleotideSequenceId':\n",
    "                    refseq_nt = prop['value']\n",
    "        if dbref['database'] == \"EMBL\":\n",
    "            embl_nt = dbref.get('id') or embl_nt\n",
    "            for prop in dbref.get('properties', []):\n",
    "                if prop['key'].lower() in ['protein sequence id', 'proteinid', 'protein_id']:\n",
    "                    embl_prot = prop['value']\n",
    "    return refseq_prot, refseq_nt, embl_nt, embl_prot\n",
    "\n",
    "\n",
    "def extract_contig_block(gb_text):\n",
    "    \"\"\"\n",
    "    Extracts the full CONTIG block from a GenBank flatfile entry as a single string.\n",
    "    \"\"\"\n",
    "    lines = gb_text.splitlines()\n",
    "    contig_lines = []\n",
    "    in_contig = False\n",
    "\n",
    "    for line in lines:\n",
    "        if line.lstrip().startswith('CONTIG'):\n",
    "            in_contig = True\n",
    "            contig_lines.append(line.strip())\n",
    "        elif in_contig and (line.startswith(' ') or line.startswith('\\t')):\n",
    "            # Continuation line (indented)\n",
    "            contig_lines.append(line.strip())\n",
    "        elif in_contig and not (line.startswith(' ') or line.startswith('\\t')):\n",
    "            # End of CONTIG block\n",
    "            break\n",
    "    return \" \".join(contig_lines) if contig_lines else \"\"\n",
    "\n",
    "\n",
    "def get_contigs_from_refseq(refseq_nt, embl_nt):\n",
    "    \"\"\"Given a RefSeq nt accession, extract all contig accessions from CONTIG field. Fallback to master accession if none.\"\"\"\n",
    "    try:\n",
    "        with Entrez.efetch(db='nuccore', id=refseq_nt, rettype='gb', retmode='text', timeout=60) as handle:\n",
    "            gb_data = handle.read()\n",
    "        \n",
    "        contig_block = extract_contig_block(gb_data)\n",
    "        if not contig_block:\n",
    "            return [refseq_nt]  # fallback single\n",
    "\n",
    "        # search for accession ids in join(xxx), e.g JFBT01000001.1\n",
    "        contigs = re.findall(r'([A-Z]{4}\\d{8}\\.\\d+)', contig_block)\n",
    "        return contigs if contigs else [embl_nt]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_contigs_from_refseq({refseq_nt}): {e}\")\n",
    "        return [embl_nt]\n",
    "    \n",
    "    \n",
    "def get_cds_list(embl_acc):\n",
    "    \"\"\"Get all CDS/gene features for a nucleotide accession.\"\"\"\n",
    "    genes = []\n",
    "    try:\n",
    "        with Entrez.efetch(db='nuccore', id=embl_acc, rettype='gbwithparts', retmode='text', timeout=500) as handle:\n",
    "            records = list(SeqIO.parse(handle, \"genbank\"))\n",
    "        for record in records:\n",
    "            for idx, feat in enumerate(record.features):\n",
    "                if feat.type == \"CDS\":\n",
    "                    gene = feat.qualifiers.get('gene', [None])[0]\n",
    "                    locus_tag = feat.qualifiers.get('locus_tag', [None])[0]\n",
    "                    product = feat.qualifiers.get('product', [None])[0]\n",
    "                    protein_id = feat.qualifiers.get('protein_id', [None])[0]\n",
    "                    # db_xref\n",
    "                    db_xrefs = feat.qualifiers.get('db_xref', [])\n",
    "                    embl_prot = None\n",
    "                    for dbx in db_xrefs:\n",
    "                        if dbx.startswith(\"EMBL:\"):\n",
    "                            embl_prot = dbx.split(\"EMBL:\")[1]\n",
    "                        if dbx.startswith(\"protein_id:\"):\n",
    "                            embl_prot = dbx.split(\"protein_id:\")[1]\n",
    "                    start = int(feat.location.start)\n",
    "                    end = int(feat.location.end)\n",
    "                    strand = feat.location.strand\n",
    "                    genes.append(dict(\n",
    "                        gene=gene,\n",
    "                        locus_tag=locus_tag,\n",
    "                        product=product,\n",
    "                        embl_prot=embl_prot,\n",
    "                        protein_id=protein_id,\n",
    "                        idx=idx,\n",
    "                        start=start,\n",
    "                        end=end,\n",
    "                        mid=(start+end)//2,\n",
    "                        contig=record.id,\n",
    "                        strand=strand\n",
    "                    ))\n",
    "        return genes\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {embl_acc}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_csv, header=None, names=['uniprot'])\n",
    "results = []\n",
    "\n",
    "for uniprot_id in tqdm(df['uniprot'], desc=\"UniProt IDs\"):\n",
    "    refseq_prot, refseq_nt, embl_nt, embl_prot = get_uniprot_mappings(uniprot_id)\n",
    "    sleep(0.2)\n",
    "\n",
    "    nucleotide_id = refseq_nt or embl_nt\n",
    "    if not nucleotide_id:\n",
    "        results.append({\n",
    "            'uniprot': uniprot_id,\n",
    "            'refseq_prot': refseq_prot or \"\",\n",
    "            'refseq_nt': refseq_nt or \"\",\n",
    "            'embl_nucleotide': embl_nt or \"\",\n",
    "            'embl_protein': embl_prot or \"\",\n",
    "            'error': 'No nucleotide sequences found'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    contigs = get_contigs_from_refseq(nucleotide_id, embl_nt)\n",
    "    sleep(0.2)\n",
    "\n",
    "    # For each contig, scan for genes and annotate which contig they're from\n",
    "    all_genes = []\n",
    "    for ctg in contigs:\n",
    "        genes = get_cds_list(ctg)\n",
    "        for g in genes:\n",
    "            g['contig'] = ctg\n",
    "        all_genes.extend(genes)\n",
    "        sleep(0.3)\n",
    "\n",
    "    # Find the hit gene by protein_id match (embl_prot), if possible\n",
    "    hit = next((g for g in all_genes if g['protein_id'] == embl_prot and embl_prot), None)\n",
    "    \n",
    "    row = dict(\n",
    "        uniprot=uniprot_id,\n",
    "        refseq_prot=refseq_prot or \"\",\n",
    "        refseq_nt=refseq_nt or \"\",\n",
    "        embl_nucleotide=embl_nt or \"\",\n",
    "        embl_protein=embl_prot or \"\",\n",
    "        total_contigs=len(contigs),\n",
    "        contig_list=','.join(contigs),\n",
    "    )\n",
    "\n",
    "    if hit:\n",
    "        row.update({\n",
    "            \"hit_gene\": f\"{hit['gene']}|{hit['locus_tag']}|{hit['start']}-{hit['end']}\",\n",
    "            \"hit_contig\": hit['contig'],\n",
    "            \"hit_start\": hit['start'],\n",
    "            \"hit_end\": hit['end'],\n",
    "            \"hit_mid\": hit['mid'],\n",
    "            \"hit_strand\": hit['strand'],\n",
    "            \"hit_protein_id\": hit['protein_id']\n",
    "        })\n",
    "        hit_idx, hit_mid, hit_contig = hit['idx'], hit['mid'], hit['contig']\n",
    "    else:\n",
    "        row[\"hit_gene\"] = \"\"\n",
    "        row[\"hit_contig\"] = \"\"\n",
    "        row[\"hit_start\"] = \"\"\n",
    "        row[\"hit_end\"] = \"\"\n",
    "        row[\"hit_mid\"] = \"\"\n",
    "        row[\"hit_strand\"] = \"\"\n",
    "        row[\"hit_protein_id\"] = \"\"\n",
    "        hit_idx, hit_mid, hit_contig = None, None, None\n",
    "\n",
    "    # Check: are ANY CDS features annotated with a gene name?\n",
    "    no_gene_names_provided = all(g['gene'] is None for g in all_genes) if all_genes else True\n",
    "    \n",
    "    # For each search gene, find closest (prefer same contig), report distances if on same contig\n",
    "    for sj in search_genes:\n",
    "        if no_gene_names_provided:\n",
    "            # No annotation present at all\n",
    "            row[f\"{sj}_gene\"] = \"no gene names provided\"\n",
    "            row[f\"{sj}_contig\"] = \"\"\n",
    "            row[f\"{sj}_dist_bp\"] = \"\"\n",
    "            row[f\"{sj}_dist_genes\"] = \"\"\n",
    "            row[f\"{sj}_start\"] = \"\"\n",
    "            row[f\"{sj}_end\"] = \"\"\n",
    "            row[f\"{sj}_strand\"] = \"\"\n",
    "            row[f\"{sj}_protein_id\"] = \"\"\n",
    "        else:\n",
    "            # Annotated genes present, see if our search gene exists\n",
    "            sj_matches = [g for g in all_genes if g['gene'] and sj.lower() == g['gene'].lower()]\n",
    "            if not sj_matches:\n",
    "                row[f\"{sj}_gene\"] = \"gene not in sequence\"\n",
    "                row[f\"{sj}_contig\"] = \"\"\n",
    "                row[f\"{sj}_dist_bp\"] = \"\"\n",
    "                row[f\"{sj}_dist_genes\"] = \"\"\n",
    "                row[f\"{sj}_start\"] = \"\"\n",
    "                row[f\"{sj}_end\"] = \"\"\n",
    "                row[f\"{sj}_strand\"] = \"\"\n",
    "                row[f\"{sj}_protein_id\"] = \"\"\n",
    "            else:\n",
    "                if hit is None:\n",
    "                    sj_best = sj_matches[0]\n",
    "                else:\n",
    "                    sj_best = min(\n",
    "                        sj_matches,\n",
    "                        key=lambda g: (g['contig'] != hit_contig, abs(g['mid']-hit_mid))\n",
    "                    )\n",
    "                row[f\"{sj}_gene\"] = f\"{sj_best['gene']}|{sj_best['locus_tag']}|{sj_best['start']}-{sj_best['end']}\"\n",
    "                row[f\"{sj}_contig\"] = sj_best['contig']\n",
    "                row[f\"{sj}_start\"] = sj_best['start']\n",
    "                row[f\"{sj}_end\"] = sj_best['end']\n",
    "                row[f\"{sj}_strand\"] = sj_best['strand']\n",
    "                row[f\"{sj}_protein_id\"] = sj_best['protein_id']\n",
    "                if hit is None or sj_best['contig'] != hit_contig:\n",
    "                    row[f\"{sj}_dist_bp\"] = \"NA\"\n",
    "                    row[f\"{sj}_dist_genes\"] = \"NA\"\n",
    "                else:\n",
    "                    row[f\"{sj}_dist_bp\"] = abs(sj_best['mid']-hit_mid)\n",
    "                    row[f\"{sj}_dist_genes\"] = sj_best['idx']-hit_idx\n",
    "\n",
    "    results.append(row)\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "print(f\"Done! Results written to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function (safe requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_requests_get(url, timeout=10, tries=3, backoff=2):\n",
    "    delay = 10\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except (requests.RequestException, Exception) as e:\n",
    "            if i >= tries-1:\n",
    "                print(f\"ERROR: {e} for {url}\")\n",
    "                return None\n",
    "            print(f\"Retrying ({i+1}): {e} for {url}, sleeping {delay}s...\")\n",
    "            sleep(delay)\n",
    "            delay *= backoff\n",
    "\n",
    "def safe_entrez_efetch(*args, tries=3, backoff=2, **kwargs):\n",
    "    delay = 10\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            return Entrez.efetch(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            if i >= tries-1:\n",
    "                print(f\"ERROR: {e} for efetch with args {args} {kwargs}\")\n",
    "                return None\n",
    "            print(f\"Retrying efetch ({i+1}) for {args}: {e}, sleeping {delay}s...\")\n",
    "            sleep(delay)\n",
    "            delay *= backoff\n",
    "\n",
    "\n",
    "def get_uniprot_mappings(uniprot_id):\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
    "    r = safe_requests_get(url)\n",
    "    if r is None:\n",
    "        print(f\"UniProt fetch failed for {uniprot_id}\")\n",
    "        return None, None, None, None\n",
    "    data = r.json()\n",
    "    refseq_prot, refseq_nt, embl_nt, embl_prot = None, None, None, None\n",
    "    for dbref in data.get('uniProtKBCrossReferences', []):\n",
    "        if dbref['database'] == \"RefSeq\":\n",
    "            refseq_prot = dbref.get('id') or refseq_prot\n",
    "            for prop in dbref.get('properties', []):\n",
    "                if prop['key'] == 'NucleotideSequenceId':\n",
    "                    refseq_nt = prop['value']\n",
    "        if dbref['database'] == \"EMBL\":\n",
    "            embl_nt = dbref.get('id') or embl_nt\n",
    "            for prop in dbref.get('properties', []):\n",
    "                if prop['key'].lower() in ['protein sequence id', 'proteinid', 'protein_id']:\n",
    "                    embl_prot = prop['value']\n",
    "    return refseq_prot, refseq_nt, embl_nt, embl_prot\n",
    "\n",
    "\n",
    "def extract_contig_block(gb_text):\n",
    "    lines = gb_text.splitlines()\n",
    "    contig_lines = []\n",
    "    in_contig = False\n",
    "    for line in lines:\n",
    "        if line.lstrip().startswith('CONTIG'):\n",
    "            in_contig = True\n",
    "            contig_lines.append(line.strip())\n",
    "        elif in_contig and (line.startswith(' ') or line.startswith('\\t')):\n",
    "            contig_lines.append(line.strip())\n",
    "        elif in_contig and not (line.startswith(' ') or line.startswith('\\t')):\n",
    "            break\n",
    "    return \" \".join(contig_lines) if contig_lines else \"\"\n",
    "\n",
    "\n",
    "def get_contigs_from_refseq(refseq_nt, embl_nt):\n",
    "    try:\n",
    "        handle = safe_entrez_efetch(db='nuccore', id=refseq_nt, rettype='gb', retmode='text', timeout=60)\n",
    "        if handle is None:\n",
    "            return [embl_nt]\n",
    "        gb_data = handle.read()\n",
    "        handle.close()\n",
    "        contig_block = extract_contig_block(gb_data)\n",
    "        if not contig_block:\n",
    "            return [refseq_nt]\n",
    "        contigs = re.findall(r'([A-Z]{4}\\d{8}\\.\\d+)', contig_block)\n",
    "        return contigs if contigs else [embl_nt]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_contigs_from_refseq({refseq_nt}): {e}\")\n",
    "        return [embl_nt]\n",
    "\n",
    "\n",
    "def get_cds_list(embl_acc, tries=3):\n",
    "    genes = []\n",
    "    delay = 1\n",
    "    for attempt in range(tries):\n",
    "        try:\n",
    "            handle = safe_entrez_efetch(db='nuccore', id=embl_acc, rettype='gbwithparts', retmode='text', timeout=500)\n",
    "            if handle is None:\n",
    "                return []\n",
    "            records = list(SeqIO.parse(handle, \"genbank\"))\n",
    "            handle.close()\n",
    "            for record in records:\n",
    "                for idx, feat in enumerate(record.features):\n",
    "                    if feat.type == \"CDS\":\n",
    "                        gene = feat.qualifiers.get('gene', [None])[0]\n",
    "                        locus_tag = feat.qualifiers.get('locus_tag', [None])[0]\n",
    "                        product = feat.qualifiers.get('product', [None])[0]\n",
    "                        protein_id = feat.qualifiers.get('protein_id', [None])[0]\n",
    "                        db_xrefs = feat.qualifiers.get('db_xref', [])\n",
    "                        embl_prot = None\n",
    "                        for dbx in db_xrefs:\n",
    "                            if dbx.startswith(\"EMBL:\"):\n",
    "                                embl_prot = dbx.split(\"EMBL:\")[1]\n",
    "                            if dbx.startswith(\"protein_id:\"):\n",
    "                                embl_prot = dbx.split(\"protein_id:\")[1]\n",
    "                        start = int(feat.location.start)\n",
    "                        end = int(feat.location.end)\n",
    "                        strand = feat.location.strand\n",
    "                        genes.append(dict(\n",
    "                            gene=gene,\n",
    "                            locus_tag=locus_tag,\n",
    "                            product=product,\n",
    "                            embl_prot=embl_prot,\n",
    "                            protein_id=protein_id,\n",
    "                            idx=idx,\n",
    "                            start=start,\n",
    "                            end=end,\n",
    "                            mid=(start+end)//2,\n",
    "                            contig=record.id,\n",
    "                            strand=strand\n",
    "                        ))\n",
    "            return genes\n",
    "        except Exception as e:\n",
    "            if attempt >= tries-1:\n",
    "                print(f\"Error parsing {embl_acc}: {e}\")\n",
    "                return []\n",
    "            print(f\"Retrying get_cds_list for {embl_acc}: {e}, sleeping {delay}s...\")\n",
    "            sleep(delay)\n",
    "            delay *= 2\n",
    "            \n",
    "            \n",
    "def save_checkpoint(df, results, output_csv):\n",
    "    df_out = pd.DataFrame(results)\n",
    "    df_out.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n[Checkpoint] Saved {len(results)} results to {output_csv}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function (safe requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:   4%|▍         | 20/471 [00:55<20:48,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 20 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:   8%|▊         | 40/471 [02:11<25:13,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 40 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  13%|█▎        | 60/471 [03:22<25:24,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 60 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  17%|█▋        | 80/471 [04:26<21:43,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 80 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  21%|██        | 100/471 [05:32<21:10,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 100 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  25%|██▌       | 120/471 [06:36<18:11,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 120 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  30%|██▉       | 140/471 [07:31<13:26,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 140 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  34%|███▍      | 160/471 [08:35<26:47,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 160 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  38%|███▊      | 180/471 [09:39<18:45,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 180 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  42%|████▏     | 200/471 [10:33<11:54,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 200 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  47%|████▋     | 220/471 [11:31<09:53,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 220 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  51%|█████     | 240/471 [12:28<10:15,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 240 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  55%|█████▌    | 260/471 [13:57<20:16,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 260 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  59%|█████▉    | 280/471 [14:54<07:05,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 280 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  64%|██████▎   | 300/471 [15:50<07:11,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 300 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  68%|██████▊   | 320/471 [16:56<06:32,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 320 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  72%|███████▏  | 340/471 [17:51<05:35,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 340 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  76%|███████▋  | 360/471 [18:44<04:19,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 360 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  81%|████████  | 380/471 [19:39<03:29,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 380 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  85%|████████▍ | 400/471 [20:39<05:54,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 400 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  89%|████████▉ | 420/471 [21:56<02:05,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 420 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  93%|█████████▎| 440/471 [22:55<01:19,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 440 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs:  98%|█████████▊| 460/471 [23:54<00:44,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 460 results to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UniProt IDs: 100%|██████████| 471/471 [24:29<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved 471 results to protein_synteny_leftSenAcluster_output.csv\n",
      "Done! Results written to protein_synteny_leftSenAcluster_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_csv, header=None, names=['uniprot'])\n",
    "results = []\n",
    "\n",
    "for i, uniprot_id in enumerate(tqdm(df['uniprot'], desc=\"UniProt IDs\")):\n",
    "    refseq_prot, refseq_nt, embl_nt, embl_prot = get_uniprot_mappings(uniprot_id)\n",
    "    sleep(0.3)\n",
    "\n",
    "    nucleotide_id = refseq_nt or embl_nt\n",
    "    if not nucleotide_id:\n",
    "        results.append({\n",
    "            'uniprot': uniprot_id,\n",
    "            'refseq_prot': refseq_prot or \"\",\n",
    "            'refseq_nt': refseq_nt or \"\",\n",
    "            'embl_nucleotide': embl_nt or \"\",\n",
    "            'embl_protein': embl_prot or \"\",\n",
    "            'error': 'No nucleotide sequences found'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    contigs = get_contigs_from_refseq(nucleotide_id, embl_nt)\n",
    "    sleep(0.3)\n",
    "\n",
    "    all_genes = []\n",
    "    for ctg in contigs:\n",
    "        genes = get_cds_list(ctg)\n",
    "        for g in genes:\n",
    "            g['contig'] = ctg\n",
    "        all_genes.extend(genes)\n",
    "        sleep(0.4)\n",
    "\n",
    "    hit = next((g for g in all_genes if g['protein_id'] == embl_prot and embl_prot), None)\n",
    "\n",
    "    row = dict(\n",
    "        uniprot=uniprot_id,\n",
    "        refseq_prot=refseq_prot or \"\",\n",
    "        refseq_nt=refseq_nt or \"\",\n",
    "        embl_nucleotide=embl_nt or \"\",\n",
    "        embl_protein=embl_prot or \"\",\n",
    "        total_contigs=len(contigs),\n",
    "        contig_list=','.join(contigs),\n",
    "    )\n",
    "\n",
    "    if hit:\n",
    "        row.update({\n",
    "            \"hit_gene\": f\"{hit['gene']}|{hit['locus_tag']}|{hit['start']}-{hit['end']}\",\n",
    "            \"hit_contig\": hit['contig'],\n",
    "            \"hit_start\": hit['start'],\n",
    "            \"hit_end\": hit['end'],\n",
    "            \"hit_mid\": hit['mid'],\n",
    "            \"hit_strand\": hit['strand'],\n",
    "            \"hit_protein_id\": hit['protein_id']\n",
    "        })\n",
    "        hit_idx, hit_mid, hit_contig = hit['idx'], hit['mid'], hit['contig']\n",
    "    else:\n",
    "        row[\"hit_gene\"] = \"\"\n",
    "        row[\"hit_contig\"] = \"\"\n",
    "        row[\"hit_start\"] = \"\"\n",
    "        row[\"hit_end\"] = \"\"\n",
    "        row[\"hit_mid\"] = \"\"\n",
    "        row[\"hit_strand\"] = \"\"\n",
    "        row[\"hit_protein_id\"] = \"\"\n",
    "        hit_idx, hit_mid, hit_contig = None, None, None\n",
    "\n",
    "    no_gene_names_provided = all(g['gene'] is None for g in all_genes) if all_genes else True\n",
    "\n",
    "    for sj in search_genes:\n",
    "        if no_gene_names_provided:\n",
    "            row[f\"{sj}_gene\"] = \"no gene names provided\"\n",
    "            row[f\"{sj}_contig\"] = \"\"\n",
    "            row[f\"{sj}_dist_bp\"] = \"\"\n",
    "            row[f\"{sj}_dist_genes\"] = \"\"\n",
    "            row[f\"{sj}_start\"] = \"\"\n",
    "            row[f\"{sj}_end\"] = \"\"\n",
    "            row[f\"{sj}_strand\"] = \"\"\n",
    "            row[f\"{sj}_protein_id\"] = \"\"\n",
    "        else:\n",
    "            sj_matches = [g for g in all_genes if g['gene'] and sj.lower() == g['gene'].lower()]\n",
    "            if not sj_matches:\n",
    "                row[f\"{sj}_gene\"] = \"gene not in sequence\"\n",
    "                row[f\"{sj}_contig\"] = \"\"\n",
    "                row[f\"{sj}_dist_bp\"] = \"\"\n",
    "                row[f\"{sj}_dist_genes\"] = \"\"\n",
    "                row[f\"{sj}_start\"] = \"\"\n",
    "                row[f\"{sj}_end\"] = \"\"\n",
    "                row[f\"{sj}_strand\"] = \"\"\n",
    "                row[f\"{sj}_protein_id\"] = \"\"\n",
    "            else:\n",
    "                if hit is None:\n",
    "                    sj_best = sj_matches[0]\n",
    "                else:\n",
    "                    sj_best = min(\n",
    "                        sj_matches,\n",
    "                        key=lambda g: (g['contig'] != hit_contig, abs(g['mid']-hit_mid))\n",
    "                    )\n",
    "                row[f\"{sj}_gene\"] = f\"{sj_best['gene']}|{sj_best['locus_tag']}|{sj_best['start']}-{sj_best['end']}\"\n",
    "                row[f\"{sj}_contig\"] = sj_best['contig']\n",
    "                row[f\"{sj}_start\"] = sj_best['start']\n",
    "                row[f\"{sj}_end\"] = sj_best['end']\n",
    "                row[f\"{sj}_strand\"] = sj_best['strand']\n",
    "                row[f\"{sj}_protein_id\"] = sj_best['protein_id']\n",
    "                if hit is None or sj_best['contig'] != hit_contig:\n",
    "                    row[f\"{sj}_dist_bp\"] = \"NA\"\n",
    "                    row[f\"{sj}_dist_genes\"] = \"NA\"\n",
    "                else:\n",
    "                    row[f\"{sj}_dist_bp\"] = abs(sj_best['mid']-hit_mid)\n",
    "                    row[f\"{sj}_dist_genes\"] = sj_best['idx']-hit_idx\n",
    "\n",
    "    results.append(row)\n",
    "\n",
    "    # --- Checkpoint save ---\n",
    "    if (i + 1) % CHECKPOINT_EVERY == 0 or (i + 1) == len(df):\n",
    "        save_checkpoint(df, results, output_csv)\n",
    "\n",
    "print(f\"Done! Results written to {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
